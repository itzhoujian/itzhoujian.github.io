<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[web日志预处理]]></title>
      <url>%2F2016%2F12%2F29%2Fweb%E6%97%A5%E5%BF%97%E9%A2%84%E5%A4%84%E7%90%86%2F</url>
      <content type="text"><![CDATA[需求：对web访问日志中的各字段识别切分去除日志中不合法的记录根据KPI统计需求，生成各类访问请求过滤数据 实现：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108/** * 对web访问日志中的各字段识别切分 去除日志中不合法的记录 根据KPI统计需求，生成各类访问请求过滤数据 定义一个bean，用来记录日志数据中的各数据字段 * @author zj * @date 2016年12月29日 上午9:54:31 */public class WebLogBean &#123; private String remote_addr;// 记录客户端的ip地址 private String remote_user;// 记录客户端用户名称,忽略属性&quot;-&quot; private String time_local;// 记录访问时间与时区 private String request;// 记录请求的url与http协议 private String status;// 记录请求状态；成功是200 private String body_bytes_sent;// 记录发送给客户端文件主体内容大小 private String http_referer;// 用来记录从那个页面链接访问过来的 private String http_user_agent;// 记录客户浏览器的相关信息 private boolean valid = true;// 判断数据是否合法 public String getRemote_addr() &#123; return remote_addr; &#125; public void setRemote_addr(String remote_addr) &#123; this.remote_addr = remote_addr; &#125; public String getRemote_user() &#123; return remote_user; &#125; public void setRemote_user(String remote_user) &#123; this.remote_user = remote_user; &#125; public String getTime_local() &#123; return time_local; &#125; public void setTime_local(String time_local) &#123; this.time_local = time_local; &#125; public String getRequest() &#123; return request; &#125; public void setRequest(String request) &#123; this.request = request; &#125; public String getStatus() &#123; return status; &#125; public void setStatus(String status) &#123; this.status = status; &#125; public String getBody_bytes_sent() &#123; return body_bytes_sent; &#125; public void setBody_bytes_sent(String body_bytes_sent) &#123; this.body_bytes_sent = body_bytes_sent; &#125; public String getHttp_referer() &#123; return http_referer; &#125; public void setHttp_referer(String http_referer) &#123; this.http_referer = http_referer; &#125; public String getHttp_user_agent() &#123; return http_user_agent; &#125; public void setHttp_user_agent(String http_user_agent) &#123; this.http_user_agent = http_user_agent; &#125; public boolean isValid() &#123; return valid; &#125; public void setValid(boolean valid) &#123; this.valid = valid; &#125; @Override public String toString() &#123; StringBuilder sBuilder = new StringBuilder(); sBuilder.append(this.valid); sBuilder.append(&quot;\001&quot;).append(this.remote_addr); sBuilder.append(&quot;\001&quot;).append(this.remote_user); sBuilder.append(&quot;\001&quot;).append(this.time_local); sBuilder.append(&quot;\001&quot;).append(this.request); sBuilder.append(&quot;\001&quot;).append(this.status); sBuilder.append(&quot;\001&quot;).append(this.body_bytes_sent); sBuilder.append(&quot;\001&quot;).append(this.http_referer); sBuilder.append(&quot;\001&quot;).append(this.http_user_agent); return sBuilder.toString(); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * 读入日志文件 进行解析处理 * @author zj * @date 2016年12月29日 上午10:23:26 */public class WebLogParseJob &#123; static class WebLogParseMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;&#123; Text k = new Text(); NullWritable v = NullWritable.get(); @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; String string = value.toString(); WebLogBean webLogBean = WebLogParser.parser(string); //记住*** if (!webLogBean.isValid()) return; k.set(webLogBean.toString()); context.write(k, v); &#125; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); Job job = Job.getInstance(conf); FileSystem fs = FileSystem.get(conf); //指定job需要的jar包路径 job.setJarByClass(WebLogParseJob.class); //job所需要的map业务类 这里没用到reduce 不用设置 或设置job.setNumReduceTasks(0) job.setMapperClass(WebLogParseMapper.class); //job.setNumReduceTasks(0); //设置最总的输出数据kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); //设置job的数据源目录和输出目录 FileInputFormat.setInputPaths(job, new Path(args[0])); Path outPath = new Path(args[1]); if (fs.exists(outPath)) &#123; fs.delete(outPath, true); &#125; FileOutputFormat.setOutputPath(job, outPath); //提交job job.waitForCompletion(true); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556/** * 解析过滤web访问日志原始记录逻辑 * @author zj * @date 2016年12月29日 上午10:05:06 */public class WebLogParser &#123; static SimpleDateFormat sdt1 = new SimpleDateFormat(&quot;dd/MMM/yyyy:HH:mm:ss&quot;,Locale.US);//因为时间是18/Sep/2013:06:52:39 月份由3为 所以3个M static SimpleDateFormat sdt2 = new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;); public static WebLogBean parser(String line)&#123; WebLogBean webLogBean = new WebLogBean(); String[] fields = line.split(&quot; &quot;); if(fields.length &gt; 11)&#123; webLogBean.setRemote_addr(fields[0]); webLogBean.setRemote_user(fields[1]); webLogBean.setTime_local(parseTime(fields[3].substring(1))); webLogBean.setRequest(fields[6]); webLogBean.setStatus(fields[8]); webLogBean.setBody_bytes_sent(fields[9]); webLogBean.setHttp_referer(fields[10]); if (fields.length &gt; 12) &#123; webLogBean.setHttp_user_agent(fields[11] + &quot; &quot; + fields[12]); &#125; else &#123; webLogBean.setHttp_user_agent(fields[11]); &#125; if (Integer.parseInt(webLogBean.getStatus()) &gt;= 400) &#123;// 大于400，HTTP错误 webLogBean.setValid(false); &#125; &#125;else &#123; webLogBean.setValid(false); &#125; return webLogBean; &#125; /** * 时间格式化 * @return String */ public static String parseTime(String dt)&#123; String timeString=&quot;&quot;; try &#123; Date parse = sdt1.parse(dt); timeString = sdt2.format(parse); &#125; catch (ParseException e) &#123; e.printStackTrace(); &#125; return timeString; &#125; public static void main(String[] args) &#123; String parseTime = parseTime(&quot;18/Sep/2013:06:49:18&quot;); System.out.println(parseTime); &#125;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[社交粉丝数据分析]]></title>
      <url>%2F2016%2F12%2F29%2F%E7%A4%BE%E4%BA%A4%E7%B2%89%E4%B8%9D%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%2F</url>
      <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091/** * 需求： * 有个好友列表数据，参考：fans.txt文件。冒号前是一个用户，冒号后是该用户的所有好友（数据中的好友关系是单向的）。 * 1、求出哪些人两两之间有共同好友，及他俩的共同好友都有谁？ * 2、求互粉的人(待实现......) * 技术点： * 1.首先考虑你要清楚原始数据和得到的最终数据是什么格式 如： * 原始数据: A:B,C,D,F,E,O B:A,C,E,K C:F,A,D,I 最终数据: A-B C,E A-C D,F B-C A 接下来反推： reduce：输出：A-B C,E A-C D,F 输入：&lt;A-B,C&gt; &lt;A-B,E&gt; map:输出：&lt;I-K,C&gt; &lt;I-C,C&gt;,&lt;I-B,C&gt; .... 输入：C I,K,C,B,G,F,H,O,D —————————————————————————————————————————————————————————— reduce: 输出：C I,K,C,B,G,F,H,O,D,&lt;好友，人，人，人.....&gt; 输入：&lt;C,A&gt;&lt;C,B&gt;&lt;C,E&gt;&lt;C,F&gt;&lt;C,G&gt;......(有序) map: 输出：&lt;B,A&gt;&lt;C,A&gt;&lt;C,B&gt;&lt;E,B&gt;...... * @author zj * @date 2016年12月28日 上午10:52:00 */public class SocialContactFansJob1 &#123; static class SharedFriendsStepOneMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt;&#123; @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; String[] fields = value.toString().split(&quot;:&quot;); String person = fields[0]; //用户 String friends = fields[1];//好友列表 for (String friend : friends.split(&quot;,&quot;)) &#123; //输出&lt;好友，人&gt;&lt;B,A&gt;&lt;C,A&gt;&lt;C,B&gt;&lt;E,B&gt;...... context.write(new Text(friend), new Text(person)); &#125; &#125; &#125; static class SharedFriendsStepOneReducer extends Reducer&lt;Text, Text, Text, Text&gt;&#123; @Override protected void reduce(Text k, Iterable&lt;Text&gt; v, Reducer&lt;Text, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; StringBuffer sb = new StringBuffer(); for (Text person : v) &#123; sb.append(person).append(&quot;,&quot;); &#125; //输出&lt;好友，人，人,人......&gt; context.write(k, new Text(sb.toString())); &#125; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration cf = new Configuration(); Job job = Job.getInstance(cf); FileSystem fs = FileSystem.newInstance(cf); //指定本地jar所在的路径 job.setJarByClass(SocialContactFansJob1.class); //指定job所需要的map/reduce业务类 job.setMapperClass(SharedFriendsStepOneMapper.class); job.setReducerClass(SharedFriendsStepOneReducer.class); //指定map的输出kv类型 //job.setMapOutputKeyClass(Text.class); //job.setMapOutputValueClass(Text.class); //指定最终输出数据kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); //指定job输入原始目录和输出目录 FileInputFormat.setInputPaths(job, new Path(args[0])); Path outPath =new Path(args[1]); if (fs.exists(outPath)) &#123; fs.delete(outPath, true); &#125; FileOutputFormat.setOutputPath(job, outPath); //提交运行 boolean b = job.waitForCompletion(true); System.exit(b?0:1); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class SocialContactFansJob2 &#123; static class SharedFriendsStepTwoMapper extends Mapper&lt;LongWritable, Text, Text, Text&gt; &#123; @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; //输入数据为上一个job的输出数据格式为 //C I,K,C,B,G,F,H,O,D String[] fields = value.toString().split(&quot;\t&quot;); String friend= fields[0]; String[] persons = fields[1].split(&quot;,&quot;); Arrays.sort(persons); //组合成&lt;I-K,C&gt; &lt;I-B,C&gt;,&lt;I-C,C&gt; for (int i = 0; i &lt; persons.length-1; i++) &#123; for (int j = i+1; j &lt; persons.length; j++) &#123; context.write(new Text(persons[i] + &quot;-&quot; + persons[j]), new Text(friend)); &#125; &#125; &#125; &#125; static class SharedFriendsStepTwoReducer extends Reducer&lt;Text, Text, Text, Text&gt;&#123; @Override protected void reduce(Text k, Iterable&lt;Text&gt; v, Reducer&lt;Text, Text, Text, Text&gt;.Context context) throws IOException, InterruptedException &#123; StringBuffer sb = new StringBuffer(); for (Text friend : v) &#123; sb.append(friend).append(&quot; &quot;); &#125; context.write(k, new Text(sb.toString())); &#125; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration cf = new Configuration(); Job job = Job.getInstance(cf); FileSystem fs = FileSystem.newInstance(cf); //指定jar所在的路径 job.setJarByClass(SocialContactFansJob2.class); //指定job所需要的map/reduce的业务类 job.setMapperClass(SharedFriendsStepTwoMapper.class); job.setReducerClass(SharedFriendsStepTwoReducer.class); //指定最终的输出数据kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(Text.class); //指定job的原始数据目录和输出目录 FileInputFormat.setInputPaths(job, new Path(args[0])); Path outPath = new Path(args[1]); if (fs.exists(outPath)) &#123; fs.delete(outPath, true); &#125; FileOutputFormat.setOutputPath(job, outPath); //提交运行 boolean b = job.waitForCompletion(true); System.exit(b?0:1); &#125;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[自定义combine]]></title>
      <url>%2F2016%2F12%2F29%2F%E8%87%AA%E5%AE%9A%E4%B9%89combine%2F</url>
      <content type="text"><![CDATA[待处理]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[自定义partitioner和排序-流量统计分析]]></title>
      <url>%2F2016%2F12%2F29%2F%E8%87%AA%E5%AE%9A%E4%B9%89partitioner%E5%92%8C%E6%8E%92%E5%BA%8F-%E6%B5%81%E9%87%8F%E7%BB%9F%E8%AE%A1%E5%88%86%E6%9E%90%2F</url>
      <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485/** * 需求： * 1、对流量日志中的用户统计总上、下行流量、总流量。 * 2、按照总流量大小倒序排序。 * 3、按照手机号的归属地，将结果数据输出到不同的省份文件中。(2,3 不能同时进行全局排序？) * 技术点： * 1、自定义javaBean用来在mapreduce中充当value，javaBean要实现WritableComparable接口，覆写compareTo方法实现排序逻辑。 * 2、需要2个job，第二个job读入第一个job输出，将flowBean作为第二个job的mmap的key。 * 3、自定义Partitioner，根据自定义的partition逻辑设置相应数量的reduce task。 * * @author zj * @date 2016年12月27日 下午2:54:00 */public class FlowBean implements WritableComparable&lt;FlowBean&gt; &#123; private long upFlow;// 上行流量 private long downFlow;// 下行流量 private long sumFlow; // 总流量 public FlowBean(long upFlow, long downFlow) &#123; this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = upFlow + downFlow; &#125; //反序列化时需要反射调用空参数的构造方法 public FlowBean() &#123; &#125; // 反序列化 @Override public void readFields(DataInput in) throws IOException &#123; upFlow = in.readLong(); downFlow = in.readLong(); sumFlow = in.readLong(); &#125; // 序列化 @Override public void write(DataOutput out) throws IOException &#123; out.writeLong(upFlow); out.writeLong(downFlow); out.writeLong(sumFlow);; &#125; @Override public int compareTo(FlowBean o) &#123; return this.sumFlow&gt;o.getSumFlow()?-1:1; &#125; @Override public String toString() &#123; return upFlow + &quot;\t&quot; + downFlow + &quot;\t&quot; + sumFlow; &#125; public void set(long upFlow, long downFlow) &#123; this.upFlow = upFlow; this.downFlow = downFlow; this.sumFlow = upFlow + downFlow; &#125; public Long getUpFlow() &#123; return upFlow; &#125; public void setUpFlow(long upFlow) &#123; this.upFlow = upFlow; &#125; public Long getDownFlow() &#123; return downFlow; &#125; public void setDownFlow(long downFlow) &#123; this.downFlow = downFlow; &#125; public Long getSumFlow() &#123; return sumFlow; &#125; public void setSumFlow(long sumFlow) &#123; this.sumFlow = sumFlow; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475public class FlowCountJob &#123; static class FlowCountMapper extends Mapper&lt;LongWritable, Text, Text, FlowBean&gt;&#123; @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, FlowBean&gt;.Context context) throws IOException, InterruptedException &#123; String[] fields = value.toString().split(&quot;\t&quot;); //将一行内容转换成String切分成字段 String phoneNum = fields[1]; //取出手机号 long upFlow = Long.parseLong(fields[fields.length-3]);//取出上行流量 long downFlow = Long.parseLong(fields[fields.length-2]); //取出下行流量 context.write(new Text(phoneNum), new FlowBean(upFlow,downFlow)); &#125; &#125; static class FlowCountReduce extends Reducer&lt;Text, FlowBean, Text, FlowBean&gt;&#123; @Override protected void reduce(Text key, Iterable&lt;FlowBean&gt; values, Reducer&lt;Text, FlowBean, Text, FlowBean&gt;.Context context) throws IOException, InterruptedException &#123; //reduce阶段会拿到&lt;183323,bean1&gt;&lt;183323,bean2&gt;&lt;183323,bean3&gt;&lt;183323,bean4&gt;....... long sum_upFlow = 0; long sum_downFlow = 0; //遍历所有的bean将上行流量、下行流量进行累加 for (FlowBean flowBean : values) &#123; sum_downFlow = flowBean.getDownFlow(); sum_upFlow = flowBean.getUpFlow(); &#125; context.write(key, new FlowBean(sum_upFlow,sum_downFlow)); &#125; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException, URISyntaxException &#123; Configuration cf = new Configuration(); FileSystem fs =FileSystem.newInstance(cf); //集群模式运行 ////客户端去操作hdfs时，是有一个用户身份的，默认情况下hdfs客户端api会从jvm中获取一个参数 -DHADOOP_USER_NAME cf.set(&quot;mapreduce.framework.name&quot;,&quot;local&quot;);// cf.set(&quot;yarn.resourcemanager.hostname&quot;, &quot;hadoop1&quot;);// cf.set(&quot;fs.defaultFS&quot;, &quot;hdfs://hadoop1:9000/&quot;); Job job = Job.getInstance(cf); job.setJarByClass(FlowCountJob.class);//指定本程序jar包所在的路径 //指定本程序job要使用的mapper/reduce业务类 job.setMapperClass(FlowCountMapper.class); job.setReducerClass(FlowCountReduce.class); //指定自定义分区和响应分区数据量的reduce task job.setPartitionerClass(ProvincePartitioner.class); job.setNumReduceTasks(5); //指定map输出的数据的kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); //指定最终输出的数据kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); //指定job输入原始目录和输出结果目录 FileInputFormat.setInputPaths(job, new Path(args[0])); Path outPath = new Path(args[1]); //判断目录是否存在 存在则删除 if (fs.exists(outPath)) &#123; fs.delete(outPath, true); &#125; FileOutputFormat.setOutputPath(job, outPath); //将job中配置的相关参数，以及job所用的java类所在的jar包，提交给yarn去运行 /*job.submit();*/ boolean b = job.waitForCompletion(true); System.exit(b?0:1); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475/** * 读取统计好的数据进行排序 * @author zj * @date 2016年12月27日 下午8:20:45 */public class FlowCountSortJob &#123; static class FlowCountSortMapper extends Mapper&lt;LongWritable, Text, FlowBean, Text&gt;&#123; FlowBean bean=new FlowBean(); Text v = new Text(); @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, FlowBean, Text&gt;.Context context) throws IOException, InterruptedException &#123; String[] fields = value.toString().split(&quot;\t&quot;); String phoneNbr = fields[0]; long sum_upFlow = Long.parseLong(fields[1]);//上行流量 long sum_downFlow = Long.parseLong(fields[2]);//下行流量 bean.set(sum_upFlow,sum_downFlow); v.set(phoneNbr); context.write(bean, v); &#125; &#125; static class FlowCountSortReducer extends Reducer&lt;FlowBean, Text, Text, FlowBean&gt;&#123; @Override protected void reduce(FlowBean k, Iterable&lt;Text&gt; v, Reducer&lt;FlowBean, Text, Text, FlowBean&gt;.Context context) throws IOException, InterruptedException &#123; context.write(v.iterator().next(), k); &#125; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration cf = new Configuration(); //集群模式// cf.set(&quot;mapreduce.framework.name&quot;, &quot;yarn&quot;);// cf.set(&quot;yarn.resourcemanager.hostname&quot;, &quot;hadoop1&quot;); FileSystem fs =FileSystem.newInstance(cf); Job job = Job.getInstance(cf); //指定本程序jar包所在的路径 job.setJarByClass(FlowCountSortJob.class); //指定本程序job使用的mapper/reduce业务类 job.setMapperClass(FlowCountSortMapper.class); job.setReducerClass(FlowCountSortReducer.class); //指定自定义分区和响应分区数据量的reduce task job.setPartitionerClass(ProvincePartitioner.class); job.setNumReduceTasks(5); //指定mapper输出数据kv类型 job.setMapOutputKeyClass(FlowBean.class); job.setMapOutputValueClass(Text.class); //指定最终输出数据kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(FlowBean.class); //指定job输入原始目录和输出结果目录 FileInputFormat.setInputPaths(job, new Path(args[0])); Path outPath = new Path(args[1]); if (fs.exists(outPath)) &#123; fs.delete(outPath, true); &#125; FileOutputFormat.setOutputPath(job, outPath); //提交运行 boolean b = job.waitForCompletion(true); System.exit(b?0:1); &#125;&#125; 1234567891011121314151617181920212223/** * 自定义分区 * @author zj * @date 2016年12月29日 下午9:35:14 */public class ProvincePartitioner extends Partitioner&lt;Text, FlowBean&gt;&#123; public static HashMap&lt;String, Integer&gt; proviceDict = new HashMap&lt;String, Integer&gt;(); static&#123; proviceDict.put(&quot;136&quot;, 0); proviceDict.put(&quot;137&quot;, 1); proviceDict.put(&quot;138&quot;, 2); proviceDict.put(&quot;139&quot;, 3); System.out.println(&quot;||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||\\\\&quot;); &#125; @Override public int getPartition(Text k, FlowBean v, int partitionNum ) &#123; String prefix = k.toString().substring(0,3); Integer proviceId = proviceDict.get(prefix); return proviceId == null?4:proviceId; &#125;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[自定义inputFormat]]></title>
      <url>%2F2016%2F12%2F29%2F%E8%87%AA%E5%AE%9A%E4%B9%89inputFormat%2F</url>
      <content type="text"><![CDATA[需求无论hdfs还是mapreduce，对于小文件都有损效率，实践中，又难免面临处理大量小文件的场景，此时，就需要有相应解决方案 分析小文件的优化无非以下几种方式：1、 在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS2、 在业务处理之前，在HDFS上使用mapreduce程序对小文件进行合并3、 在mapreduce处理时，可采用combineInputFormat提高效率 这里我采用第二种方式程序的核心机制：自定义一个InputFormat改写RecordReader，实现一次读取一个完整文件封装为KV在输出时使用SequenceFileOutPutFormat输出合并文件 实现]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[自定义outputFormat]]></title>
      <url>%2F2016%2F12%2F29%2F%E8%87%AA%E5%AE%9A%E4%B9%89outputFormat%2F</url>
      <content type="text"><![CDATA[需求现有一些原始日志需要做增强解析处理，流程：1、 从原始日志文件中读取数据2、 根据日志中的一个URL字段到外部知识库中获取信息增强到原始日志3、 如果成功增强，则输出到增强结果目录；如果增强失败，则抽取原始数据中URL字段输出到待爬清单目录 分析程序的关键点是要在一个mapreduce程序中根据数据的不同输出两类结果到不同目录，这类灵活的输出需求可以通过自定义outputformat来实现 实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293/** * 需求： * 现有一些原始日志需要做增强解析处理，流程： 1、 从原始日志文件中读取数据 2、 根据日志中的一个URL字段到外部知识库中获取信息增强到原始日志 3、 如果成功增强，则输出到增强结果目录；如果增强失败，则抽取原始数据中URL字段输出到待爬清单目录 分析： 程序的关键点是要在一个mapreduce程序中根据数据的不同输出两类结果到不同目录，这类灵活的输出需求可以通过自定义outputformat来实现 技术： 1、 在mapreduce中访问外部资源 2、 自定义outputformat，改写其中的recordwriter，改写具体输出数据的方法write() * @author zj * @date 2016年12月29日 上午11:57:38 */public class LogEnhanceJob &#123; static class LogEnhanceMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;&#123; Text kText = new Text(); NullWritable vNullWritable = NullWritable.get(); Map&lt;String, String&gt; ruleMap = new HashMap&lt;String, String&gt;(); //从数据库中加载规则信息到ruleMap中 @Override protected void setup( Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; try &#123; DBLoader.dbLoader(ruleMap); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; //在实际生产代码中，常常需要将数据处理过程中遇到的不合规数据行进行全局计数，类似这种需求可以借助mapreduce框架中提供的全局计数器来实现 Counter counter = context.getCounter(&quot;illegallogGroup&quot;, &quot;illegalLog&quot;); String string = value.toString(); String[] fields = string.split(&quot;\t&quot;); try &#123; String url = fields[28]; String content = ruleMap.get(url); //判断内容标签是否为空，如果为空，则只输出url到待爬清单；如果有值，则输出到增强日志 if (content == null) &#123; kText.set(url + &quot;\t&quot;+ &quot;tocrawl&quot; + &quot;\n&quot;); context.write(kText, vNullWritable); &#125;else &#123; kText.set(string + &quot;\t&quot; + content + &quot;\n&quot;); context.write(kText, vNullWritable); &#125; &#125; catch (Exception e) &#123; counter.increment(1); &#125; &#125; &#125; public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123; Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(conf); Job job = Job.getInstance(conf); job.setJarByClass(LogEnhanceJob.class); job.setMapperClass(LogEnhanceMapper.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); // 要控制不同的内容写往不同的目标路径，可以采用自定义outputformat的方法 job.setOutputFormatClass(LogEnhanceOutputFormat.class); FileInputFormat.setInputPaths(job, new Path(args[0])); // 尽管我们用的是自定义outputformat，但是它是继承制fileoutputformat // 在fileoutputformat中，必须输出一个_success文件，所以在此还需要设置输出path Path outPath = new Path(args[1]); if (fs.exists(outPath)) &#123; fs.delete(outPath, true); &#125; FileOutputFormat.setOutputPath(job, new Path(args[1])); // 不需要reducer job.setNumReduceTasks(0); job.waitForCompletion(true); System.exit(0); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/** * maptask或者reducetask在最终输出时，先调用OutputFormat的getRecordWriter方法拿到一个RecordWriter * 然后再调用RecordWriter的write(k,v)方法将数据写出 * @author zj * @date 2016年12月29日 下午12:57:59 */public class LogEnhanceOutputFormat extends FileOutputFormat&lt;Text, NullWritable&gt;&#123; @Override public RecordWriter&lt;Text, NullWritable&gt; getRecordWriter( TaskAttemptContext context) throws IOException, InterruptedException &#123; FileSystem fs = FileSystem.get(context.getConfiguration()); Path enhancePath = new Path(&quot;C:/hadoopTest/webLogEnhance/log.dat&quot;); Path tocrawlPath = new Path(&quot;C:/hadoopTest/webLogEnhance/tocrawlurl.dat&quot;); FSDataOutputStream enhancedOs = fs.create(enhancePath); FSDataOutputStream tocrawlOs = fs.create(tocrawlPath); return new EnhanceRecordWriter(enhancedOs, tocrawlOs); &#125; static class EnhanceRecordWriter extends RecordWriter&lt;Text, NullWritable&gt; &#123; FSDataOutputStream enhancedOs = null; FSDataOutputStream tocrawlOs = null; public EnhanceRecordWriter(FSDataOutputStream enhancedOs, FSDataOutputStream tocrawlOs) &#123; super(); this.enhancedOs = enhancedOs; this.tocrawlOs = tocrawlOs; &#125; @Override public void write(Text key, NullWritable value) throws IOException, InterruptedException &#123; String line = key.toString(); //如果写入的数据是待爬url 则写入待爬清单文件 if (line.contains(&quot;tocrawl&quot;)) &#123; tocrawlOs.write(line.getBytes()); &#125;else &#123; enhancedOs.write(line.getBytes()); &#125; &#125; @Override public void close(TaskAttemptContext context) throws IOException, InterruptedException &#123; if (tocrawlOs != null) &#123; tocrawlOs.close(); &#125; if (enhancedOs != null) &#123; enhancedOs.close(); &#125; &#125; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536/** * 访问数据库资源 * @author zj * @date 2016年12月29日 下午12:19:05 */public class DBLoader &#123; public static void dbLoader(Map&lt;String,String&gt; ruleMap) throws SQLException&#123; Connection con = null; Statement st = null; ResultSet result = null; try &#123; Class.forName(&quot;com.mysql.jdbc.Driver&quot;); con = DriverManager.getConnection(&quot;jdbc:mysql://localhost:3306/urldb&quot;, &quot;root&quot;, &quot;123456&quot;); st = con.createStatement(); result = st.executeQuery(&quot;select url,content from url_rule&quot;); while(result.next())&#123; ruleMap.put(result.getString(1), result.getString(2)); &#125; &#125; catch (ClassNotFoundException e) &#123; e.printStackTrace(); &#125;finally&#123; if (result != null) &#123; result.close(); &#125; if (st != null) &#123; st.close(); &#125; if (con != null) &#123; con.close(); &#125; &#125; &#125;&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[自定义GroupingComparator]]></title>
      <url>%2F2016%2F12%2F29%2F%E8%87%AA%E5%AE%9A%E4%B9%89GroupingComparator%2F</url>
      <content type="text"><![CDATA[需求假设有如下订单数据订单id 商品id 成交金额 123456Order_0000001 Pdt_01 222.8Order_0000001 Pdt_05 25.8Order_0000002 Pdt_03 522.8Order_0000002 Pdt_04 122.4Order_0000002 Pdt_05 722.4Order_0000003 Pdt_01 222.8 现在需要求出每一个订单中成交金额最大的一笔交易 分析1、利用“订单id和成交金额”作为key，可以将map阶段读取到的所有订单数据按照id分区，按照金额排序，发送到reduce2、在reduce端利用groupingcomparator将订单id相同的kv聚合成组，然后取第一个即是最大值 实现]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[小文件优化策略]]></title>
      <url>%2F2016%2F12%2F29%2F%E5%B0%8F%E6%96%87%E4%BB%B6%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5%2F</url>
      <content type="text"><![CDATA[1.在数据采集的时候，就将小文件或小批数据合成大文件再上传HDFS(最佳) 2.在mapreduce处理时，可采用combineInputFormat提高效率(补刀)combineInputFormat默认可以修改切片大小，来决定map tasks数量 job.setInputFormatClass(CombineTextInputFormat.class); CombineTextInputFormat.setMaxInputSplitSize(job,大小) CombineTextInputFormat.setMinInputSplitSize(job,大小)3.在业务处理之前，在HDFS上使用mapreduce程序对小文件进行合并第三种是自定义一个InputFormat 改写RecordReader对象，复写 nextKeyValue方法，getCurrentKey() getCurrentValue方法。()实现一次读取一个完整文件封装为KV 在输出时使用SequenceFileOutPutFormat输出合并文件]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mapreduce中的DistributedCache应用]]></title>
      <url>%2F2016%2F12%2F28%2FMapreduce%E4%B8%AD%E7%9A%84DistributedCache%E5%BA%94%E7%94%A8%2F</url>
      <content type="text"><![CDATA[需求假设要实现两个“表”的join操作，其中一个表数据量小，一个表很大，这种场景在实际中非常常见，比如“订单日志” join “产品信息” 原理阐述适用于关联表中有小表的情形；可以将小表分发到所有的map节点，这样，map节点就可以在本地对自己所读到的大表数据进行join并输出最终结果可以大大提高join操作的并发度，加快处理速度 技术先在mapper类中预先定义好小表，进行join并用distributedcache机制将小表的数据分发到每一个maptask执行节点，从而每一个maptask节点可以从本地加载到小表的数据，进而在本地即可实现join 实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677/** * 之前的OrderJoinProductJob有缺点：这种方式中，join的操作是在reduce阶段完成，reduce端的处理压力太大，map节点的运算负载则很低，资源利用率不高，且在reduce阶段极易产生数据倾斜 解决方案： map端join实现方式 原理阐述： 适用于关联表中有小表的情形； 可以将小表分发到所有的map节点，这样，map节点就可以在本地对自己所读到的大表数据进行join并输出最终结果，可以大大提高join操作的并发度，加快处理速度 技术： 一次加载数据库或者用distributedcache，在mapper类中预先定义好小表，进行join 不用reducer * @author zj * @date 2016年12月28日 下午8:01:49 */public class MapSideJoinJob &#123; static class MapSideJoinMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt;&#123; // 用一个hashmap来加载保存产品信息表 Map&lt;String, String&gt; pdInfoMap = new HashMap&lt;String, String&gt;(); Text k = new Text(); //setup方法是在maptask处理数据之前调用一次 可以用来做一些初始化工作 @Override protected void setup( Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; //从缓存中读取加载的文件 BufferedReader bReader = new BufferedReader(new InputStreamReader(new FileInputStream(&quot;pdts.txt&quot;))); String line; while(StringUtils.isNotBlank(line = bReader.readLine()))&#123; String[] fields = line.split(&quot;,&quot;); pdInfoMap.put(fields[0], fields[1]); &#125; bReader.close(); &#125; @Override protected void map(LongWritable key, Text value, Mapper&lt;LongWritable, Text, Text, NullWritable&gt;.Context context) throws IOException, InterruptedException &#123; String line = value.toString(); String[] fields = line.split(&quot;,&quot;); String pName = pdInfoMap.get(fields[2]); k.set(line + &quot;\t&quot; + pName); context.write(k, NullWritable.get()); &#125; &#125; public static void main(String[] args) throws IOException, URISyntaxException, ClassNotFoundException, InterruptedException &#123; Configuration cf= new Configuration(); Job job = Job.getInstance(cf); FileSystem fs = FileSystem.get(cf); job.setJarByClass(MapSideJoinMapper.class); //设置job所需要的map业务类 job.setMapperClass(MapSideJoinMapper.class); //设置最终输出kv类型 job.setOutputKeyClass(Text.class); job.setOutputValueClass(NullWritable.class); //指定job数据源目录和输出结果目录 FileInputFormat.setInputPaths(job, new Path(args[0])); Path outPath = new Path(args[1]); if (fs.exists(outPath)) &#123; fs.delete(outPath, true); &#125; FileOutputFormat.setOutputPath(job, outPath); //指定需要普通缓存的文件到所有的maptask工作目录 job.addCacheFile(new URI(&quot;file:/C:/Users/zhoujian/workspace/bigData_hadoop/ordersToP/pdts.txt&quot;));//?路径问题待解决???? //map端join的逻辑不需要reduce阶段，设置reducetask数量为0 job.setNumReduceTasks(0); //提交运行 boolean b = job.waitForCompletion(true); System.exit(b?0:1); &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899public class InfoBean implements Writable &#123; private int oId; private String date; private String pid; private int amount; private String pname; // flag=0表示这个对象是封装订单表记录 // flag=1表示这个对象是封装产品信息记录 private String flag; public String getFlag() &#123; return flag; &#125; public void setFlag(String flag) &#123; this.flag = flag; &#125; //反序列化 @Override public void readFields(DataInput in) throws IOException &#123; this.oId = in.readInt(); this.date = in.readUTF(); this.pid = in.readUTF(); this.amount = in.readInt(); this.pname = in.readUTF(); this.flag = in.readUTF(); &#125; //序列化 @Override public void write(DataOutput out) throws IOException &#123; out.writeInt(oId); out.writeUTF(date); out.writeUTF(pid); out.writeInt(amount); out.writeUTF(pname); out.writeUTF(flag); &#125; public void setInfo(int oId,String date,String pid,int amount,String pname,String flag)&#123; this.oId = oId; this.date = date; this.pid = pid; this.amount = amount; this.pname = pname; this.flag = flag; &#125; @Override public String toString() &#123; return oId + &quot;\t&quot;+date +&quot;\t&quot; + pid +&quot;\t&quot; + amount +&quot;\t&quot; + pname; &#125; public int getoId() &#123; return oId; &#125; public void setoId(int oId) &#123; this.oId = oId; &#125; public String getDate() &#123; return date; &#125; public void setDate(String date) &#123; this.date = date; &#125; public String getPid() &#123; return pid; &#125; public void setPid(String pid) &#123; this.pid = pid; &#125; public int getAmount() &#123; return amount; &#125; public void setAmount(int amount) &#123; this.amount = amount; &#125; public String getPname() &#123; return pname; &#125; public void setPname(String pname) &#123; this.pname = pname; &#125; &#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[mapreduce参数优化]]></title>
      <url>%2F2016%2F12%2F28%2Fmapreduce%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96%2F</url>
      <content type="text"><![CDATA[资源相关参数//以下参数是在用户自己的mr应用程序中配置就可以生效(1) mapreduce.map.memory.mb: 一个Map Task可使用的资源上限（单位:MB），默认为1024。如果Map Task实际使用的资源量超过该值，则会被强制杀死。(2) mapreduce.reduce.memory.mb: 一个Reduce Task可使用的资源上限（单位:MB），默认为1024。如果Reduce Task实际使用的资源量超过该值，则会被强制杀死。(3) mapreduce.map.java.opts: Map Task的JVM参数，你可以在此配置默认的java heap size等参数, e.g.“-Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc” （@taskid@会被Hadoop框架自动换为相应的taskid）, 默认值: “”(4) mapreduce.reduce.java.opts: Reduce Task的JVM参数，你可以在此配置默认的java heap size等参数, e.g.“-Xmx1024m -verbose:gc -Xloggc:/tmp/@taskid@.gc”, 默认值: “”(5) mapreduce.map.cpu.vcores: 每个Map task可使用的最多cpu core数目, 默认值: 1(6) mapreduce.reduce.cpu.vcores: 每个Reduce task可使用的最多cpu core数目, 默认值: 1 //应该在yarn启动之前就配置在服务器的配置文件中才能生效(7) yarn.scheduler.minimum-allocation-mb 1024 给应用程序container分配的最小内存(8) yarn.scheduler.maximum-allocation-mb 8192 给应用程序container分配的最大内存(9) yarn.scheduler.minimum-allocation-vcores 1(10)yarn.scheduler.maximum-allocation-vcores 32(11)yarn.nodemanager.resource.memory-mb 8192 (总内存)//shuffle性能优化的关键参数，应在yarn启动之前就配置好(12) mapreduce.task.io.sort.mb 100 //shuffle的环形缓冲区大小，默认100m(13) mapreduce.map.sort.spill.percent 0.8 //环形缓冲区溢出的阈值，默认80% 容错相关参数(1) mapreduce.map.maxattempts: 每个Map Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。(2) mapreduce.reduce.maxattempts: 每个Reduce Task最大重试次数，一旦重试参数超过该值，则认为Map Task运行失败，默认值：4。(3) mapreduce.map.failures.maxpercent: 当失败的Map Task失败比例超过该值为，整个作业则失败，默认值为0. 如果你的应用程序允许丢弃部分输入数据，则该该值设为一个大于0的值，比如5，表示如果有低于5%的Map Task失败（如果一个Map Task重试次数超过mapreduce.map.maxattempts，则认为这个Map Task失败，其对应的输入数据将不会产生任何结果），整个作业扔认为成功。(4) mapreduce.reduce.failures.maxpercent: 当失败的Reduce Task失败比例超过该值为，整个作业则失败，默认值为0.(5) mapreduce.task.timeout: Task超时时间，经常需要设置的一个参数，该参数表达的意思为：如果一个task在一定时间内没有任何进入，即不会读取新的数据，也没有输出数据，则认为该task处于block状态，可能是卡住了，也许永远会卡主，为了防止因为用户程序永远block住不退出，则强制设置了一个该超时时间（单位毫秒），默认是300000。如果你的程序对每条输入数据的处理时间过长（比如会访问数据库，通过网络拉取数据等），建议将该参数调大，该参数过小常出现的错误提示是“AttemptID:attempt_14267829456721_123456_m_000224_0 Timed out after 300 secsContainer killed by the ApplicationMaster.”。 效率和稳定性相关参数(1) mapreduce.map.speculative: 是否为Map Task打开推测执行机制，默认为false(一般不建议开启，弄不好的话 效率更低)(2) mapreduce.reduce.speculative: 是否为Reduce Task打开推测执行机制，默认为false(3) mapreduce.job.user.classpath.first &amp; mapreduce.task.classpath.user.precedence：当同一个class同时出现在用户jar包和hadoop jar中时，优先使用哪个jar包中的class，默认为false，表示优先使用hadoop jar中的class。(4) mapreduce.input.fileinputformat.split.minsize: FileInputFormat做切片时的最小切片大小，(5)mapreduce.input.fileinputformat.split.maxsize: FileInputFormat做切片时的最大切片大小(切片的默认大小就等于blocksize，即 134217728)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[shell脚本定时采集日志数据]]></title>
      <url>%2F2016%2F12%2F24%2Fshell%E8%84%9A%E6%9C%AC%E5%AE%9A%E6%97%B6%E9%87%87%E9%9B%86%E6%97%A5%E5%BF%97%E6%95%B0%E6%8D%AE%2F</url>
      <content type="text"><![CDATA[12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#!/bin/bash#set java envexport JAVA_HOME=/usr/lib/java/jdk1.7.0_79/export JRE_HOME=$&#123;JAVA_HOME&#125;/jreexport CLASSPATH=.:$&#123;JAVA_HOME&#125;/lib:$&#123;JRE_HOME&#125;/libexport PATH=$&#123;JAVA_HOME&#125;/bin:$PATH#set hadoop envexport HADOOP_HOME=/app/hadoop/hadoop-2.2.0export PATH=$&#123;HADOOP_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/sbin:$PATH#日志文件存放目录log_src_dir=/home/hadoop/logs/log/#待上传文件存放目录log_toupload_dir=/home/hadoop/logs/toupload/#hdfs目录名称关联当前系统时间datedir=`date +%Y%m%d`#日志文件上传到hdfs的根目录,（每天生产文件夹需要修改）hdfs_root_dir=/data/clickLog/`date +%Y%m%d`#打印环境变量信息echo &quot;envs:hadoop_home:$HADOOP_HOME&quot;#读取日志文件的目录，判断是否需要上传文件echo &quot;log_src_dir:&quot;$log_src_dirls $log_src_dir | while read fileNamedo if [[ &quot;$fileName&quot; == access.log.* ]];then date=`date +%Y_%m_%d_%H_%M_%S` #将文件移动到待上传目录并重命名 echo &quot;moving $log_src_dir$fileName to $log_toupload_dir&quot;xxxxx_click_log_$fileName&quot;$date&quot; mv $log_src_dir$fileName $log_toupload_dir&quot;xxxxx_click_log_$fileName&quot;$date #将待上传文件path写入一个列表文件willDong.$date echo $log_toupload_dir&quot;xxxxx_click_log_$fileName&quot;$date &gt;&gt; $log_toupload_dir&quot;willDoing.&quot;$date fidone#找到列表文件的willDoingls $log_toupload_dir | grep will | grep -v &quot;_COPY_&quot; | grep -v &quot;_DONE_&quot; | while read linedo #打印信息 echo &quot;toupload is in file :&quot;$line #将待上传文件列表willDoing改名为willDoing_COPY_ mv $log_toupload_dir$line $log_toupload_dir$line&quot;_COPY_&quot; #读列表文件willDling_COPY_的内容 一个一个文件上传 cat $log_toupload_dir$line&quot;_COPY_&quot; | while read line do #打印信息 echo &quot;puting ...... $line to hdfs path... $hdfs_root_dir&quot; hadoop fs -put $line $hdfs_root_dir done mv $log_toupload_dir$line&quot;_COPY_&quot; $log_toupload_dir$line&quot;_DONE_&quot;done 最后crontab 做定时任务]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[HDFS原理]]></title>
      <url>%2F2016%2F12%2F24%2FHDFS%E5%8E%9F%E7%90%86%2F</url>
      <content type="text"><![CDATA[(注：HDFS适合用来做数据分析，并不适合用来做网盘应用，因为，不便修改，延迟大，网络开销大，成本太高) HDFS的工作机制hdfs写数据1.和namenode通信请求上传文件，namenode验证目标是否存在，文件是否存在，是否可以上传。2.如果可以上传，客户端请求的第一个block快该上传到哪些datanode，需要从请求返回的namenode列表中获取。3.假如返回A、B、C、3个namenode，客户端会在一台上传数据，A收到请求会继续调用B，然后B调用C，将真个pipeline建立完成，逐级返回客户端(本质上是一个RPC调用，建立pipeline,以packet为单位)4.当一个block传输完成之后，client再次请求namenode上传第二个block的服务器。 hdfs读数据1.和namenode进行通信查询元数据，找到block对应的datanode信息。2.挑选一台datanode服务器建立连接(就近原则，然后随机)，建立socket流3.datanode开始发送数据(从磁盘读出数据放入流中，以packet(64K)为单位校验)4.客户端以packet为单位接收，先存入本地，在写入目标文件。 NAMENODE工作机制元数据管理1.namenode对数据的管理采用了3种形式：2.内存元数据管理(meta.data)3.磁盘镜像fsimage文件管理4.数据操作日志文件edits管理 元数据存储当客户端对hdfs系统上的文件进行新增或修改时，操作记录首先被记录edits文件中，当客户端操作成功后，相应的元数据会更新到内存meta.data。当checkpoint被触发后，secondNameNode后到namenode工作目录中抓取fsimage文件和edits文件，并进行merge，重新替换之前的fsimage文件。 namenode和secondary namenode的工作目录存储结构完全相同，所以，当namenode故障退出需要重新恢复时，可以从secondary namenode的工作目录中将fsimage拷贝到namenode的工作目录，以恢复namenode的元数据。 DATANODE的工作机制存储管理用户的文件块数据定期向namenode汇报自身所持有的block信息（通过心跳信息上报）（这点很重要，因为，当集群中发生某些block副本失效时，集群如何恢复block初始副本数量的问题） Datanode掉线判断时限参数，HDFS默认的超时时长为10分钟+30秒。如果定义超时时间为timeout，则超时时长的计算公式为： timeout = 2 heartbeat.recheck.interval(单位毫秒) + 10 dfs.heartbeat.interval(单位秒)。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[FTP接口数据采集]]></title>
      <url>%2F2016%2F12%2F24%2FFTP%E6%8E%A5%E5%8F%A3%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86%2F</url>
      <content type="text"><![CDATA[需求从外部购买数据，数据提供方会实时将数据推送到6台FTP服务器上，我方部署6台接口采集机来对接采集数据，并上传到HDFS中 提供商在FTP上生成数据的规则是以小时为单位建立文件夹(2016-03-11-10)，每分钟生成一个文件（00.dat,01.data,02.dat,……..） 提供方不提供数据备份，推送到FTP服务器的数据如果丢失，不再重新提供，且FTP服务器磁盘空间有限，最多存储最近10小时内的数据 由于每一个文件比较小，只有150M左右，因此，我方在上传到HDFS过程中，需要将15分钟时段的数据合并成一个文件上传到HDFS 为了区分数据丢失的责任，我方在下载数据时最好进行校验 未完待续 详情资料正在整理中……]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[网站或APP点击流日志数据挖掘系统]]></title>
      <url>%2F2016%2F12%2F23%2F%E7%BD%91%E7%AB%99%E6%88%96APP%E7%82%B9%E5%87%BB%E6%B5%81%E6%97%A5%E5%BF%97%E6%95%B0%E6%8D%AE%E6%8C%96%E6%8E%98%E7%B3%BB%E7%BB%9F%2F</url>
      <content type="text"><![CDATA[需求描述“Web点击流日志”包含着网站运营很重要的信息，通过日志分析，我们可以知道网站的访问量，哪个网页访问人数最多，哪个网页最有价值，广告转化率、访客的来源信息，访客的终端信息等。 数据来源数据主要是用户的点击行为记录，获取方式通常是在页面内预埋js代码，为页面想要监听的标签绑定事件，只要用户点击或移动到标签就会触发ajax请求到后台servlet，用log4j记录下日志信息，在后台服务器nginx、tomcat不断产生增长日志文件。 数据量分析对于一般的中型网站(10W的PV以上)，每天会产生1G以上的web日志文件。对于大型或超大型网站,可能每小时就会产生10G的数据量。具体来说，比如某电子商务网站，在线团购业务。每日PV数100w，独立IP数5w。用户通常在工作日上午10:00-12:00和下午15:00-18:00访问量最大。日间主要是通过PC端浏览器访问，休息日及夜间通过移动设备访问较多。网站搜索浏量占整个网站的80%，PC用户不足1%的用户会消费，移动用户有5%会消费。 对于日志的这种规模的数据，用HADOOP进行日志分析，是最适合不过的了。 流程图解析 数据采集：定制开发采集程序，或使用开源框架FLUME数据预处理：定制开发mapreduce程序运行于hadoop集群数据仓库技术：基于hadoop之上的Hive数据导出：基于hadoop的sqoop数据导入导出工具数据可视化：定制开发web程序或使用kettle(etl工具)等产品整个过程的流程调度：hadoop生态圈中的oozie工具或其他类似开源产品 项目技术架构图 推荐系统架构]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[zookeeper]]></title>
      <url>%2F2016%2F12%2F22%2Fzookeeper%2F</url>
      <content type="text"><![CDATA[是什么分布式协调服务框架，那么怎么理解分布式，其实就是一个完整的应用或功能或业务被分成若干个独立的部分或子业务，部署在不同的服务器上，所有部分去共同完成一个功能。优点：负载由单个节点转移到多个，提高效率缓解压力 避免了单个节点失效，整个系统崩溃的危险 提高利用率，子业务可以被反复使用而协调服务说白了就是为其它应用程序服务的。zookeeper本身也是分布式的(半数以上节点存活就能提供服务) 功能总的来说有2个管理用户提交的数据(存储或读取)为数据提供监听服务 集群角色分配原理(选举)以3台zookeepr为例，当启动第一个服务启动时发现配置文件中由3个server就会通过paxos算法进行投票，发现集群中没有leader，并且只有自己一个节点，就会投票给自己。当第二个服务启动后也发现集群中没有leader但是有一个folwer就会投给自己和第一个服务各一票，第一个服务也发现新节点也会重新投票，投给自己和对方一票，这时票数时2:2由于节点数已经过半所以paxos根据2个节点的myid号大小，将大的变为leader，其他节点在启动就会以这个leader为主，leader在维护各个folwer中的数据(这里由些偏差，待更新……) java客户端操作及监听器原理 public class SimpleZkClient { private static final String CONNECTSTRING = “192.168.25.61:2181,192.168.25.62:2181,192.168.25.63:2181”; private static final int SESSIONTIMEOUT = 200000; ZooKeeper zkClient = null; @Before public void init() throws IOException { // 初始化 Watcher监听节点的变化 此监听可以被后面的使用 zkClient = new ZooKeeper(CONNECTSTRING, SESSIONTIMEOUT, new Watcher() { @Override public void process(WatchedEvent event) { System.out.println(event.getType() + &quot;---------- &quot; + event.getPath()); try { // 因为只会监听一次，实际业务场景要时时监听 zkClient.getChildren(&quot;/&quot;, true); } catch (Exception e) { e.printStackTrace(); } } }); } // 创建节点到zk中 @Test public void testCreate() throws KeeperException, InterruptedException { // 1.znode路径及名称 2。节点的内容 3。安全策略 4。创建节点的模式（有4种 瞬时（带序号） 持久） zkClient.create(&quot;/uuuu&quot;, &quot;test&quot;.getBytes(), Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); } // 判断zode是否存在 返回许多元数据stat @Test public void testExit() throws KeeperException, InterruptedException { Stat stat = zkClient.exists(&quot;/&quot;, false); System.out.println(stat == null ? &quot;not exists&quot; : &quot;exists&quot;); } // 获取zk中的子节点 @Test public void testGet() throws KeeperException, InterruptedException { // true 表示给&quot;/&quot;节点加监听事件 用上一个监听 List&lt;String&gt; list = zkClient.getChildren(&quot;/&quot;, true); for (String string : list) { System.out.println(string); } Thread.sleep(Long.MAX_VALUE); } // 获取zode的数据 @Test public void getData() throws KeeperException, InterruptedException{ //1.znode的路径名 2.是否监听 3.获取版本 默认最新的 byte[] data = zkClient.getData(&quot;/uuuu&quot;, false, null); System.out.println(new String(data)); } //删除znode @Test public void deleteZnode() throws InterruptedException, KeeperException{ //参数-1，表示删除所有版本 zkClient.delete(&quot;/uuuu&quot;, -1); } //修改znode @Test public void setData() throws KeeperException, InterruptedException{ zkClient.setData(&quot;/test&quot;, &quot;i miss tangwei&quot;.getBytes(), -1); } } 其实所谓的监听就是服务端和客户端的通信(底层socket协议或rpc协议 )，zkClient有个连接zkServer的线程，当调用某个方法操作znode的时候如:调用getClildren()时会想zk集群中传递客户端的ip、port、path（监听的路径），zk会保存这些信息，当监听的路径发生改变时zk集群会根据以上信息找到zkClient的listern线程，listern线程调用process方法触发事件反馈给客户端处理。 应用场景主从协调(HA功能)、统一配置管理、统一名称服务(dubbo服务注册)、服务节点动态上下线、分布式共享锁 分布式应用服务器上下线动态感知程序开发需求：客户端实时洞察到服务器的变化(宕机、添加)解决思路：首先，服务器启动时就要向zk中注册Ephemeral node 信息(包括服务器地址，节点名，序列号……),然后，客户端启动时就要去getChildren获取当前在线服务器列表，并注册监听。当服务器某个节点挂掉之后临时znode会被删除，zk就会通知客户端的监听线程，客户端监听到上下znode变化事件就会调用process方法，在方法中重新获取在线服务器列表在监听。 分布式共享锁程序开发在集中式系统中有冲突、线程问题用sychronized锁机制，在分布式中通常会想到第三方去解决 如zookeeper。需求：很多客户端都去请求共享资源(网络接口)，可能会有冲突情况，怎么解决解决思路：客户端到zk中去注册锁信息(uuid或其它序号)，每次客户端请求共享资源时都会通过zk去请求，zk会根据各个客户端的锁信息如序号取最小值(或最大值)，获取资源成功后删除锁信息，这是触发监听事件通知其他客户端在获取资源，删除的客户端重新生成序号在排队等待。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[JVM浅谈]]></title>
      <url>%2F2016%2F12%2F19%2FJVM%E6%B5%85%E8%B0%88%2F</url>
      <content type="text"></content>
    </entry>

    
    <entry>
      <title><![CDATA[常见易混淆命令]]></title>
      <url>%2F2016%2F12%2F19%2F%E5%B8%B8%E8%A7%81%E6%98%93%E6%B7%B7%E6%B7%86%E5%91%BD%E4%BB%A4(%E6%8C%81%E7%BB%AD%E6%9B%B4%E6%96%B0%E4%B8%AD......)%2F</url>
      <content type="text"></content>
    </entry>

    
    <entry>
      <title><![CDATA[SSH协议]]></title>
      <url>%2F2016%2F12%2F17%2FSSH%E5%8D%8F%E8%AE%AE%2F</url>
      <content type="text"><![CDATA[浅述简单点说，ssh是网络安全外壳协议。用于计算机之间的加密协议，即使信息被截获密码也不会泄露（为什么自己去百度，这里不做重点） 在大数据领域中ssh用于安全登录，被称为免密码登录。 通常用法ssh-keygen [-t][-p] rsa/dsa(默认rsa算法可以去掉参数t)此命令会在.ssh文件夹下生产id_rsa id_rsa.pub或id_dsa id_dsa.pub,即私钥 公钥 实现免密码登录1.通过scp 公钥 用户名@主机名:/.ssh 将公钥拷贝到另一个主机中2.在另一台主机中 通过cat 公钥 &gt; /.ssh/authorized_keys 进行授权操作。上面2步可以简化为一步ssh-copy-id 实现拷贝公钥的同时进行了授权操作，建议优先使用。 聊聊ssh验证机制ssh有2种身份验证机制1.用户名+密码（用xshell或secureCRT连接使用ssh支持这种机制）2.密钥验证目标主机会跟根据授权文件中是否有请求方的公钥信息，如果有则根据对方公钥生成加密信息，再将加密信息发给请求连接方，请求连接方根据自己的私钥去解密生成解密文件在发送给目标主机，目标主机判断正确则允许连接。 常见的连接错误ssh_exchange_identification: Connection closed by remote host方法一.把SSH连接数改大方法二.检查/etc/hosts.deny和/etc/hosts.allow里面是否屏蔽了某些帐户]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[企业级服务器]]></title>
      <url>%2F2016%2F12%2F16%2F%E4%BC%81%E4%B8%9A%E7%BA%A7%E6%9C%8D%E5%8A%A1%E5%99%A8%2F</url>
      <content type="text"><![CDATA[LZ接触过hp系列的服务器，这里只对hp-DL580、hp-DL388、hp-DL380、hp-P4500等型号浅谈. 名词解释企业级：属于高档服务器,具有高内存带宽，大容量热插拔硬盘和热插拔电源，具有超强的数据处理能力机架式：机架式服务器的外形看来不像计算机，而像交换机.结构(几U)：U是一种表示服务器外部尺寸的单位,是unit的缩略语,1U=4.445cm 注意：CPU数量绝对不是CPU核心 1个CPU叫单处理器，2个CPU叫双处理器。1个CPU可以是双核货单核。 HP-DL380（此款已停产）此款服务器，售价在2-3W之间，内存标配为12G,可扩展到192G ,硬盘标配为584GB hp-DL388此款服务器，售价在2W左右，内存标配为32G,可扩展到768G ,硬盘标配为584GB HP-DL580此款服务器，售价在5.5W左右，内存标配为32G 可以最大扩展到2T内存以上，硬盘标配为4T。 HP-P4500此款系列服务器绝对高配了，售价在10W-20W之间，最大存储可达到24T 附带真实工作环境中的机房拓扑图]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[谈谈程序员的几个阶段]]></title>
      <url>%2F2016%2F12%2F15%2F%E8%B0%88%E8%B0%88%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E5%87%A0%E4%B8%AA%E9%98%B6%E6%AE%B5%2F</url>
      <content type="text"><![CDATA[关于程序员的几个阶段每个程序员、或者说每个工作者都应该有自己的职业规划，如果看到这里的朋友没有自己的职业规划，希望你可以思考一下自己的将来。LZ常常思考自己的未来,鉴于文笔拙劣，阅历浅薄。以下内容摘录于互联网，LZ拿出来，自勉、共勉之。 第一阶段：三年我认为三年对于程序员来说是第一个门槛，这个阶段将会淘汰掉一批不适合写代码的人。这一阶段，我们走出校园，迈入社会，成为一名程序员，正式从书本上的内容迈向真正的企业级开发。我们知道如何团队协作、如何使用项目管理工具、项目版本如何控制、我们写的代码如何测试如何在线上运行等等，积累了一定的开发经验，也对代码有了一定深入的认识，是一个比较纯粹的Coder的阶段 第二阶段：五年五年又是区分程序员的第二个门槛。有些人在三年里，除了完成工作，在空余时间基本不会研究别的东西，这些人永远就是个Coder，年纪大一些势必被更年轻的人给顶替；有些人在三年里，除了写代码之外，还热衷于研究各种技术实现细节、看了N多好书、写一些博客、在Github上分享技术，这些人在五年后必然具备在技术上独当一面的能力并且清楚自己未来的发展方向，从一个Coder逐步走向系统分析师或是架构师，成为项目组中不可或缺的人物 第三阶段：十年十年又是另一个门槛了，转行或是继续做一名程序员就在这个节点上。如果在前几年就抱定不转行的思路并且为之努力的话，那么在十年的这个节点上，有些人必然成长为一名对行业有着深入认识、对技术有着深入认识、能从零开始对一个产品进行分析的程序员，这样的人在公司基本担任的都是CTO、技术专家、首席架构师等最关键的职位，这对于自己绝对是一件荣耀的事，当然老板在经济上也绝不会亏待你 工作浅谈我认为，随着你工作年限的增长、对生活对生命认识的深入，应当不断思考三个问题：1、我到底适不适合当一名程序员？2、我到底应不应该一辈子以程序员为职业？3、我对编程到底持有的是一种什么样的态度，是够用就好呢还是不断研究？最终，明确自己的职业规划，对自己的规划负责并为之努力。 关于项目经验LZ在网上经常看到一些别的朋友有提出项目经验的问题，依照LZ面试的感觉来说，面试主要看几点：项目经验+基本技术+个人潜力（也就是值不值得培养）。 关于项目经验，我认为并发编程网的创始人方腾飞老师讲的一段话非常好：介绍产品时面试官会考察应聘者的沟通能力和思考能力，我们大部分情况都是做产品的一个功能或一个模块，但是即使是这样，自己有没有把整个系统架构或产品搞清楚，并能介绍清楚，为什么做这个系统？这个系统的价值是什么？这个系统有哪些功能？优缺点有哪些？如果让你重新设计这个系统你会如何设计？ 我觉得这就已经足以概括了。也许你仅仅工作一年，也许你做的是项目中微不足道的模块，当然这些一定是你的劣势且无法改变，但是如何弥补这个劣势，从方老师的话中我总结几点：1、明确你的项目到底是做什么的，有哪些功能2、明确你的项目的整体架构，在面试的时候能够清楚地画给面试官看并且清楚地指出从哪里调用到哪里、使用什么方式调用3、明确你的模块在整个项目中所处的位置及作用4、明确你的模块用到了哪些技术，更好一些的可以再了解一下整个项目用到了哪些技术 在你无法改变自己的工作年限、自己的不那么有说服力的项目经验的情况下（这一定是扣分项），可以通过这种方式来一定程度上地弥补并且增进面试官对你的好感度。 关于奔三程序员之后转行的反驳。讲到了奔三程序员的困惑，大致说的是三十岁之后程序员要转行之类的云云.]]></content>
    </entry>

    
  
  
</search>
